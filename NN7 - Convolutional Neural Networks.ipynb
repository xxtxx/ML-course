{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>html, body{overflow-y: visible !important} .CodeMirror{min-width:100% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:100%; line-height:1.0; overflow: visible;} .output_subarea pre{width:100%}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preamble import *\n",
    "HTML('''<style>html, body{overflow-y: visible !important} .CodeMirror{min-width:100% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:100%; line-height:1.0; overflow: visible;} .output_subarea pre{width:100%}</style>''') # For slides\n",
    "#HTML('''<style>html, body{overflow-y: visible !important} .output_subarea{font-size:100%; line-height:1.0; overflow: visible;}</style>''') # For slides\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "- Introduction and Motivation\n",
    "- Artificial Neuron\n",
    "- Gradient Descent\n",
    "- Backpropagation\n",
    "- Perceptron\n",
    "- Multilayered Perceptron\n",
    "- MLP Classification\n",
    "- Model Design\n",
    "- Optimization\n",
    "\n",
    "- **Convolutional Neural Network**\n",
    "- Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Mnist-digits](images/mnistdigits.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Image data \n",
    "    - How is it different from other types of data?\n",
    "![Image MNIST](images/MNIST-Matrix.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Speech data\n",
    "![Speech data](images/speech.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### High Dimensionality\n",
    "#### Local Correlations\n",
    "\n",
    "#### Convolutional Neural Networks (CNN) - utilize the local correlation property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/conv0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/conv-0-0-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/conv-0-0-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/conv0-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Parameters re-use\n",
    "![](images/conv0-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Input Visualization\n",
    "\n",
    "![Block of the image](images/conv1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "#### Conv Nets operate on volumes\n",
    "    - Take volumes of activations and produce volumes of activations\n",
    "\n",
    "#### MLPs work with vectors\n",
    "    - Take vectors of activations and produce vectors of activations\n",
    "\n",
    "#### Convolution Dimensionality\n",
    "    - for 2D convolution the volumes of activations are 3D tensor\n",
    "    - for 3D convolutions the volumes are 4D tensor\n",
    "    - for 1D convolutions the volumes are 2D matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### The input goes through a filter (receptive field)\n",
    "\n",
    "![Image volume plus filter](images/conv2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Image volume plus filter](images/conv3.png)\n",
    "#### The depth of the filter is equal to the depth of the image\n",
    "#### Discrete convolution\n",
    "    - Neuron computes: Afine transformation + non-linearity\n",
    "### Convolutional layer\n",
    "   - Hyper Parameters: filter dimensions, stride, padding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Image volume plus filter](images/conv4.png)\n",
    "  \n",
    "#### Fully convolving the input -> produces an activation map "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/conv5.png)\n",
    "\n",
    "#### Second neuron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/conv6.png)\n",
    "\n",
    "#### Third neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/conv7.png)\n",
    "\n",
    "#### Combined representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/conv8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/conv9.png)\n",
    "\n",
    "### Convolutional layer\n",
    "- Accepts: \n",
    "    - $W_1 \\times H_1 \\times D_1$\n",
    "- Outputs: \n",
    "    - $W_2 = (W_1 - F + 2P) / S + 1$\n",
    "    - $H_2 = (H_1 - F + 2P) / S + 1$\n",
    "    - $D_2 = K$\n",
    "- Where:\n",
    "    - $F$ is the filter size\n",
    "    - $P$ is the padding size\n",
    "    - $S$ is the stride\n",
    "    - $K$ is the layer depth (number of neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Depiction of the layers of the CNN\n",
    "\n",
    "![](images/conv10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sequential processing of information\n",
    "![Biological inspiration for Convolutions](images/biology-inspiration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training -> Gradient Decent\n",
    "\n",
    "### $$\\frac{\\partial L}{\\partial \\theta} $$\n",
    "\n",
    "### Back propagation\n",
    "![Back prop node](images/backprop-node2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Convolutional Neuron (Step 1)\n",
    "![Conv](images/backprop-conv-forward1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Convolutional Neuron (Step 2)\n",
    "![Conv](images/backprop-conv-forward2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Convolutional Neuron (Step 3)\n",
    "\n",
    "![Conv](images/backprop-conv-forward3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolutional Neuron Forward pass\n",
    "![Conv](images/backprop-conv-forward4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Conv](images/backprop-conv-forward5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Conv](images/backprop-conv-forward6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolutional Neuron Backward pass\n",
    "![Conv](images/backprop-conv-back0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Backward pass on $w_1$ (Step 1)\n",
    "![Conv](images/backprop-conv-back1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Backward pass on $w_1$ (Step 2)\n",
    "\n",
    "![Conv](images/backprop-conv-back2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Backward pass on $w_1$ (Step 3)\n",
    "\n",
    "![Conv](images/backprop-conv-back3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Backward pass on $x_1$ (Step 1)\n",
    "\n",
    "![Conv](images/backprop-conv-back4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Backward pass on $x_1$ (Step 2)\n",
    "\n",
    "![Conv](images/backprop-conv-back5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Propagating activation forward\n",
    "#### Vector form\n",
    "![Conv](images/backprop-conv-vector-forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Propagating gradient backward\n",
    "#### Vector form\n",
    "\n",
    "![Conv](images/backprop-conv-back.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Propagating gradient backward 2D\n",
    "#### Vector form 2D\n",
    "\n",
    "![Conv](images/backprop-conv-vector2d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Backprop through a convolutional layer\n",
    "- The grad of each parameter is spec. pre-activations times the grad of the loss wrt to post activations\n",
    "    - $\\sum_n^{|x| - |w| +1}\\frac{\\partial L}{\\partial y_n}x_{n+i-1}$\n",
    "The gradient flows in blocks back analogously as the activations flow forward\n",
    "- However the convolutional operation is done backwards as well. This can be achieved by doing the convolutional operations in the right way with a flipped filter\n",
    "    - $\\delta_i = \\sum_{i=1}^{|w|}\\frac{\\partial L}{\\partial y_{n-i+1}}w_i$\n",
    "    - $\\delta = \\frac{\\partial L}{\\partial y}*flip(w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolutional Network\n",
    "#### Architectural Depiction\n",
    "\n",
    "![](images/conv11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Convolutional Network\n",
    "#### Architectural Depiction\n",
    "\n",
    "![](images/conv12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Subsampling\n",
    "\n",
    "#### Maxpooling\n",
    "![Max Pooling](images/maxpool.png)\n",
    "\n",
    "- Field of view\n",
    "- Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Backpropagation Maxpooling\n",
    "$$\n",
    "a(x)= max(x), \\frac{\\partial a(x)}{\\partial x_i} \\begin{cases}\n",
    "1,\\ if\\ x_i=max(x)\\\\\n",
    "0,\\ otherwise\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Subsampling\n",
    "\n",
    "#### Average Pooling\n",
    "$$\n",
    "a(x)= \\frac{1}{m}\\sum_m(x), \\frac{\\partial a(x)}{\\partial x} = \\frac{1}{m}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/conv13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CNN model\n",
    "\n",
    "#### Input\n",
    "\n",
    "#### Convolutional Layers\n",
    "\n",
    "#### Flattening\n",
    "\n",
    "#### MLP\n",
    "\n",
    "#### Output Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/conv14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Regularization\n",
    "\n",
    "![](images/conv15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CNN Execution\n",
    "- MNIST dataset\n",
    "- Model:\n",
    "![Mnist model](images/mnist-conv-net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Input Layer\n",
    "![](images/cnn-input.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/conv-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Layer 1: \n",
    "![](images/conv-1-relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Layer 1: Parameters\n",
    "![](images/conv-1-weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Layer 2: Pooling\n",
    "![](images/pool-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Layer 3: Convolutional Activations\n",
    "![](images/conv-2-relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Layer 3: Weights\n",
    "![](images/conv-2-weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Layer 4: Pooling\n",
    "![](images/pool-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Layer 5: Fully conected\n",
    "![](images/fc-relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Layer 6: Softmax\n",
    "![](images/fc-softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### LeNet-5\n",
    "\n",
    "![LeNet-5](images/lenet5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example Alex Net\n",
    "\n",
    "![Alex Net](images/alexnet.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### VGG Network\n",
    "3x3 stride 1 \n",
    "maxpool 2x2 stride 2\n",
    "\n",
    "24Mil parameters 94MB of information\n",
    "\n",
    "![VGG Architecture](images/vgg-architecture.png)\n",
    "\n",
    "![VGG Network](images/vgg-network.png)\n",
    "\n",
    "\n",
    "Memory for computation vs. parameters of the model\n",
    "\n",
    "Recent developments. You can get rid of the fully connected layers (size of the model decreases). Average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "GoogLeNet\n",
    "\n",
    "![GoogLeNet](images/googlenet.png)\n",
    "\n",
    "\n",
    "12x less parameters than Alex\n",
    "\n",
    "GoogleNet 6.67%\n",
    "\n",
    "Human level performance would be about 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ResNet \n",
    "![Resnet ](images/resnet.png)\n",
    "\n",
    "Expermmets with up to 157 layers\n",
    "\n",
    "Ensamble of ResNet models 3.57% Error\n",
    "2-3 weeks on a 8 GPU machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Image Analysis\n",
    "\n",
    "#### Object detection\n",
    "    => Classification\n",
    "![Image from other slides](images/cnn-classification.png)\n",
    "\n",
    "#### Object localization\n",
    "    => Classification + Regression\n",
    "    => Multi-output models\n",
    "    => Different lossfunction, gradient propagation\n",
    "![Image from other slides](images/cnn-localization.png)\n",
    "\n",
    "#### Object segmentation\n",
    "    => Pixel classification\n",
    "    => Tensor output\n",
    "    => Complex loss function\n",
    "![Image from other slides](images/cnn-segmentation.png)\n",
    "\n",
    "#### Filtering\n",
    "    => Image output\n",
    "    => Tensor output\n",
    "    => Complex loss function\n",
    "![Image from other slides](images/cnn-filtering.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Visualize what the network has learned\n",
    "    - Low level filters\n",
    "    - Mid level\n",
    "    - Visualize the activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What we've learned so far..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Augmentation\n",
    "    - Another regularizer\n",
    "    - Opportunity to inject expert knowledge\n",
    "    \n",
    "#### Running example\n",
    "\n",
    "##### Radio graphs - XRay Images\n",
    "- Classification\n",
    "![XRay images](images/cnn-augmentation.png)\n",
    "- What can we do to augment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MNIST Conv Net (keras implementation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 255s - loss: 0.3438 - acc: 0.8953 - val_loss: 0.0794 - val_acc: 0.9755\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 274s - loss: 0.1143 - acc: 0.9665 - val_loss: 0.0554 - val_acc: 0.9829\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 254s - loss: 0.0850 - acc: 0.9745 - val_loss: 0.0452 - val_acc: 0.9851\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 248s - loss: 0.0716 - acc: 0.9787 - val_loss: 0.0387 - val_acc: 0.9866\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 250s - loss: 0.0621 - acc: 0.9811 - val_loss: 0.0361 - val_acc: 0.9877\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 250s - loss: 0.0563 - acc: 0.9839 - val_loss: 0.0335 - val_acc: 0.9883\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 249s - loss: 0.0518 - acc: 0.9848 - val_loss: 0.0352 - val_acc: 0.9888\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 249s - loss: 0.0463 - acc: 0.9861 - val_loss: 0.0300 - val_acc: 0.9898\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 334s - loss: 0.0433 - acc: 0.9871 - val_loss: 0.0312 - val_acc: 0.9896\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 277s - loss: 0.0418 - acc: 0.9871 - val_loss: 0.0293 - val_acc: 0.9898\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 275s - loss: 0.0399 - acc: 0.9879 - val_loss: 0.0303 - val_acc: 0.9900\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 284s - loss: 0.0358 - acc: 0.9886 - val_loss: 0.0281 - val_acc: 0.9909\n",
      "Test loss: 0.0280949982703\n",
      "Test accuracy: 0.9909\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MNIST Conv Net (tensorflow implementation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 20155.593750, Training Accuracy= 0.28125\n",
      "Iter 2560, Minibatch Loss= 7833.395508, Training Accuracy= 0.60156\n",
      "Iter 3840, Minibatch Loss= 7338.788086, Training Accuracy= 0.64844\n",
      "Iter 5120, Minibatch Loss= 3983.789551, Training Accuracy= 0.77344\n",
      "Iter 6400, Minibatch Loss= 2830.088867, Training Accuracy= 0.86719\n",
      "Iter 7680, Minibatch Loss= 3048.401611, Training Accuracy= 0.80469\n",
      "Iter 8960, Minibatch Loss= 1529.779297, Training Accuracy= 0.86719\n",
      "Iter 10240, Minibatch Loss= 1694.179932, Training Accuracy= 0.91406\n",
      "Iter 11520, Minibatch Loss= 2246.948975, Training Accuracy= 0.83594\n",
      "Iter 12800, Minibatch Loss= 1749.305420, Training Accuracy= 0.91406\n",
      "Iter 14080, Minibatch Loss= 2533.301758, Training Accuracy= 0.89844\n",
      "Iter 15360, Minibatch Loss= 2099.632324, Training Accuracy= 0.90625\n",
      "Iter 16640, Minibatch Loss= 1146.863892, Training Accuracy= 0.92188\n",
      "Iter 17920, Minibatch Loss= 1577.505127, Training Accuracy= 0.88281\n",
      "Iter 19200, Minibatch Loss= 992.006714, Training Accuracy= 0.92969\n",
      "Iter 20480, Minibatch Loss= 1649.223755, Training Accuracy= 0.89844\n",
      "Iter 21760, Minibatch Loss= 1183.769531, Training Accuracy= 0.92969\n",
      "Iter 23040, Minibatch Loss= 675.028198, Training Accuracy= 0.93750\n",
      "Iter 24320, Minibatch Loss= 859.713196, Training Accuracy= 0.94531\n",
      "Iter 25600, Minibatch Loss= 1426.632812, Training Accuracy= 0.89844\n",
      "Iter 26880, Minibatch Loss= 2303.080566, Training Accuracy= 0.87500\n",
      "Iter 28160, Minibatch Loss= 1456.352783, Training Accuracy= 0.88281\n",
      "Iter 29440, Minibatch Loss= 575.207275, Training Accuracy= 0.95312\n",
      "Iter 30720, Minibatch Loss= 1457.252808, Training Accuracy= 0.91406\n",
      "Iter 32000, Minibatch Loss= 811.190308, Training Accuracy= 0.92969\n",
      "Iter 33280, Minibatch Loss= 726.508057, Training Accuracy= 0.94531\n",
      "Iter 34560, Minibatch Loss= 377.169037, Training Accuracy= 0.95312\n",
      "Iter 35840, Minibatch Loss= 733.970825, Training Accuracy= 0.94531\n",
      "Iter 37120, Minibatch Loss= 86.490646, Training Accuracy= 0.98438\n",
      "Iter 38400, Minibatch Loss= 760.666626, Training Accuracy= 0.91406\n",
      "Iter 39680, Minibatch Loss= 550.637451, Training Accuracy= 0.96094\n",
      "Iter 40960, Minibatch Loss= 349.918335, Training Accuracy= 0.96094\n",
      "Iter 42240, Minibatch Loss= 864.463013, Training Accuracy= 0.92969\n",
      "Iter 43520, Minibatch Loss= 1110.746582, Training Accuracy= 0.90625\n",
      "Iter 44800, Minibatch Loss= 339.101379, Training Accuracy= 0.96875\n",
      "Iter 46080, Minibatch Loss= 363.225433, Training Accuracy= 0.96094\n",
      "Iter 47360, Minibatch Loss= 112.951172, Training Accuracy= 0.97656\n",
      "Iter 48640, Minibatch Loss= 992.423950, Training Accuracy= 0.92969\n",
      "Iter 49920, Minibatch Loss= 736.899597, Training Accuracy= 0.92188\n",
      "Iter 51200, Minibatch Loss= 640.641968, Training Accuracy= 0.96094\n",
      "Iter 52480, Minibatch Loss= 448.796387, Training Accuracy= 0.93750\n",
      "Iter 53760, Minibatch Loss= 786.762695, Training Accuracy= 0.92969\n",
      "Iter 55040, Minibatch Loss= 798.130737, Training Accuracy= 0.92969\n",
      "Iter 56320, Minibatch Loss= 684.936707, Training Accuracy= 0.90625\n",
      "Iter 57600, Minibatch Loss= 364.592255, Training Accuracy= 0.98438\n",
      "Iter 58880, Minibatch Loss= 265.106537, Training Accuracy= 0.96094\n",
      "Iter 60160, Minibatch Loss= 683.509521, Training Accuracy= 0.91406\n",
      "Iter 61440, Minibatch Loss= 804.516418, Training Accuracy= 0.94531\n",
      "Iter 62720, Minibatch Loss= 143.375656, Training Accuracy= 0.96094\n",
      "Iter 64000, Minibatch Loss= 392.629486, Training Accuracy= 0.94531\n",
      "Iter 65280, Minibatch Loss= 527.701050, Training Accuracy= 0.92188\n",
      "Iter 66560, Minibatch Loss= 324.764465, Training Accuracy= 0.97656\n",
      "Iter 67840, Minibatch Loss= 631.441772, Training Accuracy= 0.93750\n",
      "Iter 69120, Minibatch Loss= 293.203003, Training Accuracy= 0.97656\n",
      "Iter 70400, Minibatch Loss= 1057.060303, Training Accuracy= 0.92969\n",
      "Iter 71680, Minibatch Loss= 428.655548, Training Accuracy= 0.94531\n",
      "Iter 72960, Minibatch Loss= 640.031616, Training Accuracy= 0.92188\n",
      "Iter 74240, Minibatch Loss= 498.485107, Training Accuracy= 0.92969\n",
      "Iter 75520, Minibatch Loss= 423.276978, Training Accuracy= 0.95312\n",
      "Iter 76800, Minibatch Loss= 115.904999, Training Accuracy= 0.96875\n",
      "Iter 78080, Minibatch Loss= 134.409576, Training Accuracy= 0.97656\n",
      "Iter 79360, Minibatch Loss= 1211.671021, Training Accuracy= 0.95312\n",
      "Iter 80640, Minibatch Loss= 312.669769, Training Accuracy= 0.97656\n",
      "Iter 81920, Minibatch Loss= 172.990112, Training Accuracy= 0.96094\n",
      "Iter 83200, Minibatch Loss= 356.914185, Training Accuracy= 0.94531\n",
      "Iter 84480, Minibatch Loss= 130.602325, Training Accuracy= 0.96875\n",
      "Iter 85760, Minibatch Loss= 180.066101, Training Accuracy= 0.97656\n",
      "Iter 87040, Minibatch Loss= 256.264984, Training Accuracy= 0.96875\n",
      "Iter 88320, Minibatch Loss= 285.939392, Training Accuracy= 0.95312\n",
      "Iter 89600, Minibatch Loss= 498.928040, Training Accuracy= 0.96875\n",
      "Iter 90880, Minibatch Loss= 79.094299, Training Accuracy= 0.98438\n",
      "Iter 92160, Minibatch Loss= 218.652542, Training Accuracy= 0.96875\n",
      "Iter 93440, Minibatch Loss= 217.960114, Training Accuracy= 0.96094\n",
      "Iter 94720, Minibatch Loss= 483.060333, Training Accuracy= 0.92969\n",
      "Iter 96000, Minibatch Loss= 105.938507, Training Accuracy= 0.97656\n",
      "Iter 97280, Minibatch Loss= 290.152344, Training Accuracy= 0.96094\n",
      "Iter 98560, Minibatch Loss= 283.727539, Training Accuracy= 0.95312\n",
      "Iter 99840, Minibatch Loss= 403.778015, Training Accuracy= 0.96875\n",
      "Iter 101120, Minibatch Loss= 387.062927, Training Accuracy= 0.94531\n",
      "Iter 102400, Minibatch Loss= 355.027222, Training Accuracy= 0.95312\n",
      "Iter 103680, Minibatch Loss= 14.395844, Training Accuracy= 0.99219\n",
      "Iter 104960, Minibatch Loss= 200.731567, Training Accuracy= 0.96875\n",
      "Iter 106240, Minibatch Loss= 258.934692, Training Accuracy= 0.97656\n",
      "Iter 107520, Minibatch Loss= 580.618042, Training Accuracy= 0.93750\n",
      "Iter 108800, Minibatch Loss= 783.748535, Training Accuracy= 0.92969\n",
      "Iter 110080, Minibatch Loss= 490.572510, Training Accuracy= 0.95312\n",
      "Iter 111360, Minibatch Loss= 196.086380, Training Accuracy= 0.95312\n",
      "Iter 112640, Minibatch Loss= 45.372742, Training Accuracy= 0.98438\n",
      "Iter 113920, Minibatch Loss= 333.222717, Training Accuracy= 0.98438\n",
      "Iter 115200, Minibatch Loss= 262.596161, Training Accuracy= 0.95312\n",
      "Iter 116480, Minibatch Loss= 458.363098, Training Accuracy= 0.92969\n",
      "Iter 117760, Minibatch Loss= 113.962494, Training Accuracy= 0.97656\n",
      "Iter 119040, Minibatch Loss= 472.177765, Training Accuracy= 0.95312\n",
      "Iter 120320, Minibatch Loss= 300.955109, Training Accuracy= 0.96875\n",
      "Iter 121600, Minibatch Loss= 357.518524, Training Accuracy= 0.97656\n",
      "Iter 122880, Minibatch Loss= 324.303040, Training Accuracy= 0.96094\n",
      "Iter 124160, Minibatch Loss= 351.568756, Training Accuracy= 0.95312\n",
      "Iter 125440, Minibatch Loss= 356.875275, Training Accuracy= 0.93750\n",
      "Iter 126720, Minibatch Loss= 297.963867, Training Accuracy= 0.97656\n",
      "Iter 128000, Minibatch Loss= 215.975311, Training Accuracy= 0.97656\n",
      "Iter 129280, Minibatch Loss= 238.265228, Training Accuracy= 0.96875\n",
      "Iter 130560, Minibatch Loss= 359.970947, Training Accuracy= 0.95312\n",
      "Iter 131840, Minibatch Loss= 318.360657, Training Accuracy= 0.95312\n",
      "Iter 133120, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 134400, Minibatch Loss= 363.938354, Training Accuracy= 0.96094\n",
      "Iter 135680, Minibatch Loss= 149.373413, Training Accuracy= 0.97656\n",
      "Iter 136960, Minibatch Loss= 172.584351, Training Accuracy= 0.96875\n",
      "Iter 138240, Minibatch Loss= 337.201935, Training Accuracy= 0.94531\n",
      "Iter 139520, Minibatch Loss= 182.271057, Training Accuracy= 0.97656\n",
      "Iter 140800, Minibatch Loss= 25.481613, Training Accuracy= 0.98438\n",
      "Iter 142080, Minibatch Loss= 273.726868, Training Accuracy= 0.96094\n",
      "Iter 143360, Minibatch Loss= 216.207993, Training Accuracy= 0.97656\n",
      "Iter 144640, Minibatch Loss= 292.444458, Training Accuracy= 0.94531\n",
      "Iter 145920, Minibatch Loss= 260.769989, Training Accuracy= 0.95312\n",
      "Iter 147200, Minibatch Loss= 361.610779, Training Accuracy= 0.95312\n",
      "Iter 148480, Minibatch Loss= 47.024826, Training Accuracy= 0.98438\n",
      "Iter 149760, Minibatch Loss= 104.013702, Training Accuracy= 0.98438\n",
      "Iter 151040, Minibatch Loss= 52.151718, Training Accuracy= 0.97656\n",
      "Iter 152320, Minibatch Loss= 189.956284, Training Accuracy= 0.98438\n",
      "Iter 153600, Minibatch Loss= 204.803040, Training Accuracy= 0.96875\n",
      "Iter 154880, Minibatch Loss= 87.329910, Training Accuracy= 0.95312\n",
      "Iter 156160, Minibatch Loss= 40.136871, Training Accuracy= 0.99219\n",
      "Iter 157440, Minibatch Loss= 40.046989, Training Accuracy= 0.97656\n",
      "Iter 158720, Minibatch Loss= 164.694901, Training Accuracy= 0.96875\n",
      "Iter 160000, Minibatch Loss= 288.782288, Training Accuracy= 0.95312\n",
      "Iter 161280, Minibatch Loss= 223.221710, Training Accuracy= 0.96875\n",
      "Iter 162560, Minibatch Loss= 32.547241, Training Accuracy= 0.98438\n",
      "Iter 163840, Minibatch Loss= 31.965027, Training Accuracy= 0.99219\n",
      "Iter 165120, Minibatch Loss= 9.050339, Training Accuracy= 0.99219\n",
      "Iter 166400, Minibatch Loss= 241.904663, Training Accuracy= 0.96875\n",
      "Iter 167680, Minibatch Loss= 140.977478, Training Accuracy= 0.98438\n",
      "Iter 168960, Minibatch Loss= 219.999512, Training Accuracy= 0.96875\n",
      "Iter 170240, Minibatch Loss= 195.477493, Training Accuracy= 0.96875\n",
      "Iter 171520, Minibatch Loss= 65.414696, Training Accuracy= 0.98438\n",
      "Iter 172800, Minibatch Loss= 530.105713, Training Accuracy= 0.96094\n",
      "Iter 174080, Minibatch Loss= 150.237701, Training Accuracy= 0.97656\n",
      "Iter 175360, Minibatch Loss= 309.860474, Training Accuracy= 0.98438\n",
      "Iter 176640, Minibatch Loss= 107.623375, Training Accuracy= 0.97656\n",
      "Iter 177920, Minibatch Loss= 145.523438, Training Accuracy= 0.98438\n",
      "Iter 179200, Minibatch Loss= 315.156677, Training Accuracy= 0.97656\n",
      "Iter 180480, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 181760, Minibatch Loss= 357.206848, Training Accuracy= 0.93750\n",
      "Iter 183040, Minibatch Loss= 234.445465, Training Accuracy= 0.98438\n",
      "Iter 184320, Minibatch Loss= 115.685081, Training Accuracy= 0.98438\n",
      "Iter 185600, Minibatch Loss= 173.139221, Training Accuracy= 0.96094\n",
      "Iter 186880, Minibatch Loss= 63.115082, Training Accuracy= 0.98438\n",
      "Iter 188160, Minibatch Loss= 28.938911, Training Accuracy= 0.98438\n",
      "Iter 189440, Minibatch Loss= 212.562698, Training Accuracy= 0.97656\n",
      "Iter 190720, Minibatch Loss= 339.022949, Training Accuracy= 0.95312\n",
      "Iter 192000, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 193280, Minibatch Loss= 100.832047, Training Accuracy= 0.99219\n",
      "Iter 194560, Minibatch Loss= 240.516846, Training Accuracy= 0.96094\n",
      "Iter 195840, Minibatch Loss= 191.021912, Training Accuracy= 0.99219\n",
      "Iter 197120, Minibatch Loss= 247.198486, Training Accuracy= 0.98438\n",
      "Iter 198400, Minibatch Loss= 185.221649, Training Accuracy= 0.96094\n",
      "Iter 199680, Minibatch Loss= 36.471909, Training Accuracy= 0.99219\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.972656\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print (\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print (\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
