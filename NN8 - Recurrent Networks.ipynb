{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>html, body{overflow-y: visible !important} .output_subarea{font-size:100%; line-height:1.0; overflow: visible;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preamble import *\n",
    "#HTML('''<style>html, body{overflow-y: visible !important} .CodeMirror{min-width:100% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:100%; line-height:1.0; overflow: visible;} .output_subarea pre{width:100%}</style>''') # For slides\n",
    "HTML('''<style>html, body{overflow-y: visible !important} .output_subarea{font-size:100%; line-height:1.0; overflow: visible;}</style>''') # For slides\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "- Introduction and Motivation\n",
    "- Artificial Neuron\n",
    "- Gradient Descent\n",
    "- Backpropagation\n",
    "- Perceptron\n",
    "- Multilayered Perceptron\n",
    "- MLP Classification\n",
    "- Model Design\n",
    "- Optimization\n",
    "\n",
    "- Convolutional Neural Network\n",
    "- **Recurrent Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequential data\n",
    "- So far: Independent and Identically Distributed\n",
    "- Similar to the image data that had spatial correlation sequential data has dependencies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    - Data points have dependencies on previous datapoints\n",
    "    - Example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequential data\n",
    "- Measurements of processes in time\n",
    "\n",
    "#### Example:\n",
    "- Working of the human hearth:\n",
    "![ECG](images/rnn/ecg01.png)\n",
    "\n",
    "#### Should take between .06 - .1s\n",
    "- Any longer may indicate abnormality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Sample of data sequence\n",
    "\n",
    "![ECG2](images/rnn/ecg02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Feature detector for the Q, S, R\n",
    "\n",
    "- Can we use a Conv net?\n",
    "\n",
    "![Image of CNN seq](images/rnn/ecg03-rnn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Windowing\n",
    "- Window size?\n",
    "\n",
    "![Image of CNN seq](images/rnn/ecg03-rnn-01.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Location #2\n",
    "\n",
    "![Image of CNN seq](images/rnn/ecg03-rnn-02.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Faster sequence\n",
    "\n",
    "![Image of CNN seq](images/rnn/ecg03-rnn-03.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Location #3\n",
    "\n",
    "![Image of CNN seq](images/rnn/ecg03-rnn-04.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Sequential processing\n",
    "- Step 1\n",
    "\n",
    "![Image of CNN seq](images/rnn/ecg03-rnn-05.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Step 2\n",
    "\n",
    "![Image of CNN seq](images/rnn/ecg03-rnn-06.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Step 3\n",
    "\n",
    "![Image of CNN seq](images/rnn/ecg03-rnn-07.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Feature detector for the Q, and S, and R\n",
    "- Remember the the point of Q\n",
    "- Remember the point of S\n",
    "- Remember the point of R\n",
    "- Count the distance from Q to S to R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequential Data\n",
    "\n",
    "- Dependencies between the datapoints\n",
    "    - Example:\n",
    "        - Words in a sentence.\n",
    "        - Sentences in a paragraph.\n",
    "\n",
    "- Understanding text with a CNN is difficult\n",
    "    - The CNN needs to have feature detectors for all combinations of words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Auto regressive models\n",
    "Simple prediction model based on previous datapoints\n",
    "![Auto Regressive](images/rnn/autoreg01.png)\n",
    "\n",
    "$$x_t = w_0 x_{t-1} + w_1 x_t-2 + w_2$$\n",
    "$$x_t = G_\\theta(x_{t-1}, x_{t-2}, ...)$$\n",
    "\n",
    "- Fixed size input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- no parameter reuse\n",
    "- input size dimensions\n",
    "\n",
    "- Furthermore, we would like to deal with variable lenght sequences\n",
    "\n",
    "- Evenmore, we would like the output to be variable lenght \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). \n",
    "\n",
    "Not only that: These models perform this mapping using a fixed amount of computational steps (e.g. the number of layers in the model).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Recurrent Neural Network\n",
    "- Ideally, the model sees each input only once\n",
    "- Has a memory\n",
    "- Can map correlations over time\n",
    "\n",
    "![RNN](images\\rnn\\rnn01.png)\n",
    "\n",
    "$$h_t= W \\phi ( h_{t-1}) + U x_t$$\n",
    "$$y_t= V \\phi (h_t)$$\n",
    "- Turing complete model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This can in programming terms be interpreted as running a fixed program with certain inputs and some internal variables. Viewed this way, RNNs essentially describe programs. In fact, it is known that RNNs are Turing-Complete in the sense that they can to simulate arbitrary programs (with proper weights).   \n",
    "\n",
    "\n",
    "RNN computation. So how do these things work? At the core, RNNs have a deceptively simple API: They accept an input vector x and give you an output vector y. However, crucially this output vector’s contents are influenced not only by the input you just fed in, but also on the entire history of inputs you’ve fed in in the past. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Delay edge\n",
    "\n",
    "![RNN 02](images/rnn/rnn02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Diagram unrolled\n",
    "![RNN 02](images/rnn/rnn03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recurrent Neural Network Model\n",
    "\n",
    "![RNN 02](images/rnn/vanilla-rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $a^{(t)}=b+Wh^{(t-1)}+Ux^{(t)}$\n",
    "- $h^{(t)}=\\tanh(a^{(t)})$\n",
    "- $o^{(t)}=C+Vh^{(t)}$\n",
    "- $\\hat{y}^{(t)}=softmax(o^{(t)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Cells unrolled\n",
    "![RNN](images/rnn/vanilla-rnn-cell-unrolled-0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![RNN](images/rnn/vanilla-rnn-cell-unrolled-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### What can we do with a RNN?\n",
    "\n",
    "\n",
    "1. Define tasks \n",
    "    seq 2 seq\n",
    "    seq classification\n",
    "    data point to sequence\n",
    "\n",
    "![RNN applications](images/rnn/rnn-app-0.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![RNN applications](images/rnn/rnn-app-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![RNN applications](images/rnn/rnn-app-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![RNN applications](images/rnn/rnn-app-3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "#### Backpropagation\n",
    "\n",
    "### Sequence to Sequence\n",
    "\n",
    "#### Model computation unrolled\n",
    "#### Apply Backpropagation\n",
    "- Backpropagation through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/rnn/rnn-unrolled-0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/rnn/rnn-unrolled-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/rnn/rnn-unrolled-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Backpropagation through time\n",
    "\n",
    "$$ L = \\sum_t L_t$$\n",
    "$$\\frac{\\partial L}{\\partial L_t}=1$$\n",
    "\n",
    "$$\\nabla_{O_t} L = \\frac{\\partial L}{\\partial O_t} = \\frac{\\partial L}{\\partial L_t}\\frac{\\partial L_t}{\\partial O_t} =  crossentropy (\\hat{y}, {y})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/rnn/rnn-bptt-0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\nabla_{h_\\tau} L = V^{\\top} \\nabla_{O_{\\tau}} L_{\\tau} $$\n",
    "$$ \\nabla_{h_t} L = \\Big( \\frac{\\partial h_{t+1}}{\\partial h_t} \\Big)^{\\top} (\\nabla_{h_{t+1}} L) + \\Big(\\frac{\\partial O_t}{\\partial h_t}\\Big)^{\\top} \\nabla_{O_{t}} L $$\n",
    "\n",
    "$$ \\frac{\\partial h_{t+1}}{\\partial h_t} = W^{\\top} diag(\\phi'(h_{t+1}))$$\n",
    "$$ \\frac{\\partial h_{t}}{\\partial h_k} = \\prod_{i=k+1}^t W^{\\top} diag(\\phi'(h_{i-1}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### RNN challenges\n",
    "\n",
    "- Long term dependencies (Hochreiter 1991 Bengio 1994)\n",
    "- Vanishing Gradient\n",
    "- Exploding Gradient\n",
    "- Jacobian terms multiply many time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "(modelling a sequence)\n",
    "Explain where the neurons are\n",
    "\n",
    "(Predicting the next element)\n",
    "(Sequence to vector (class) output)\n",
    "(Sequence to sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sequence to sequence mapping\n",
    "- Sequence of symbols come in as input\n",
    "- Sequence of symbols come in as output\n",
    "\n",
    "- The goal of the models it to map one sequence to another\n",
    "\n",
    "- Fixed alphabet\n",
    "- Define the problem as classification\n",
    "- Output is energies + softmax\n",
    "- Loss is cross entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Computing the gradient\n",
    "Backpropagation through time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Computing the gradient through a recurrent neural network is straightforward.\n",
    "One simply applies the generalized back-propagation algorithm to the unrolled computational graph. \n",
    "\n",
    "No specialized algorithms are necessary.\n",
    "Gradients obtained by back-propagation may then be used with any general-purposegradient-based techniques to train an RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The gradient computation involvesperforming a forward propagation pass moving left to right through our illustrationof the unrolled graph in ﬁgure 10.3, followed by a backward propagation passmoving right to left through the graph. The runtime isO(τ) and cannot be reducedby parallelization because the forward propagation graph is inherently sequential;each time step may only be computed after the previous one. States computedin the forward pass must be stored until they are reused during the backwardpass, so the memory cost is alsoO(τ). The back-propagation algorithm appliedto the unrolled graph withO(τ) cost is calledback-propagation through timeor BPTT and is discussed further in section 10.2.2. The network with recurrencebetween hidden units is thus very powerful but also expensive to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanishing gradient\n",
    "sum goes over t\n",
    "same jacobian is multiplied many times in the chain rule\n",
    "if the gradient is less than one is vanishes\n",
    "if its larger than one it explodes\n",
    "(different from vanishing gradient in feed forward networks, you can normilize there, here same parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Gating\n",
    "\n",
    "Protect the state of the RNN\n",
    "\n",
    "Rather than updating the state with each datapoint\n",
    "- Learn when to update, given the input and the previous hidden state\n",
    "- What to update given the input and the previous state\n",
    "\n",
    "- Even more so, what to remove (forget)\n",
    "\n",
    "- What to add into the memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Long short term memory\n",
    "![LSTM](images/rnn/lstm-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forget gate \n",
    "\n",
    "Long short term memory\n",
    "![LSTM](images/rnn/lstm-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Add gate\n",
    "\n",
    "![LSTM](images/rnn/lstm-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output gate\n",
    "Long short term memory\n",
    "![LSTM](images/rnn/lstm-01.png)\n",
    "\n",
    "- what we are outputing based on the cell content\n",
    "- filtered by the output gate \n",
    "- sigmod of the hidden state and the input\n",
    "\n",
    "the output is propagate to the next step as well "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN Addition Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.engine.training import _slice_arrays\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, recurrent\n",
    "import numpy as np\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    '''\n",
    "    Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    '''\n",
    "    def __init__(self, chars, maxlen):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def encode(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            X[i, self.char_indices[c]] = 1\n",
    "        return X\n",
    "\n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in X)\n",
    "\n",
    "\n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n",
      "Vectorization...\n",
      "(45000, 21, 12)\n",
      "(45000, 11, 12)\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the model and dataset\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 10\n",
    "INVERT = True\n",
    "# Try replacing GRU, or SimpleRNN\n",
    "#RNN = recurrent.LSTM\n",
    "RNN = recurrent.SimpleRNN\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars, MAXLEN)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that X+Y == Y+X (hence the sorting)\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if INVERT:\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    X[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, maxlen=DIGITS + 1)\n",
    "\n",
    "# Shuffle (X, y) in unison as the later parts of X will almost all be larger digits\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over\n",
    "split_at = int(len(X) - len(X) / 10)\n",
    "(X_train, X_val) = (_slice_arrays(X, 0, split_at), _slice_arrays(X, split_at))\n",
    "(y_train, y_val) = (y[:split_at], y[split_at:])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "# note: in a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, nb_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "# For the decoder's input, we repeat the encoded input for each time step\n",
    "model.add(RepeatVector(DIGITS + 1))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer\n",
    "for _ in range(LAYERS):\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# For each of step of the output sequence, decide which character should be chosen\n",
    "model.add(TimeDistributed(Dense(len(chars))))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 27s - loss: 1.2160 - acc: 0.5473 - val_loss: 1.2096 - val_acc: 0.5463\n",
      "Q 89839+3              \n",
      "T 89842      \n",
      "\u001b[91m☒\u001b[0m 89834      \n",
      "---\n",
      "Q 940152232+3          \n",
      "T 940152235  \n",
      "\u001b[91m☒\u001b[0m 940112232  \n",
      "---\n",
      "Q 1+24843549           \n",
      "T 24843550   \n",
      "\u001b[91m☒\u001b[0m 448444444  \n",
      "---\n",
      "Q 468095498+9625344    \n",
      "T 477720842  \n",
      "\u001b[91m☒\u001b[0m 469464414  \n",
      "---\n",
      "Q 955907+832053653     \n",
      "T 833009560  \n",
      "\u001b[91m☒\u001b[0m 999333111  \n",
      "---\n",
      "Q 426267+56171         \n",
      "T 482438     \n",
      "\u001b[91m☒\u001b[0m 425140     \n",
      "---\n",
      "Q 219058096+2593       \n",
      "T 219060689  \n",
      "\u001b[91m☒\u001b[0m 219055969  \n",
      "---\n",
      "Q 606+634540           \n",
      "T 635146     \n",
      "\u001b[91m☒\u001b[0m 644343     \n",
      "---\n",
      "Q 81+89554             \n",
      "T 89635      \n",
      "\u001b[91m☒\u001b[0m 85559      \n",
      "---\n",
      "Q 268+42               \n",
      "T 310        \n",
      "\u001b[91m☒\u001b[0m 248        \n",
      "---\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "11904/45000 [======>.......................] - ETA: 19s - loss: 1.1977 - acc: 0.5521"
     ]
    }
   ],
   "source": [
    "# Train the model each generation and show predictions against the validation dataset\n",
    "for iteration in range(1, 200):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=1,\n",
    "              validation_data=(X_val, y_val))\n",
    "    ###\n",
    "    # Select 10 samples from the validation set at random so we can visualize errors\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(X_val))\n",
    "        rowX, rowy = X_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowX, verbose=0)\n",
    "        q = ctable.decode(rowX[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if INVERT else q)\n",
    "        print('T', correct)\n",
    "        print(colors.ok + '☑' + colors.close if correct == guess else colors.fail + '☒' + colors.close, guess)\n",
    "        print('---')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
