{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>html, body{overflow-y: visible !important} .CodeMirror{min-width:100% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:100%; line-height:1.0; overflow: visible;} .output_subarea pre{width:100%}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preamble import *\n",
    "HTML('''<style>html, body{overflow-y: visible !important} .CodeMirror{min-width:100% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:100%; line-height:1.0; overflow: visible;} .output_subarea pre{width:100%}</style>''') # For slides\n",
    "#HTML('''<style>html, body{overflow-y: visible !important} .output_subarea{font-size:100%; line-height:1.0; overflow: visible;}</style>''') # For slides\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "- Introduction and Motivation\n",
    "- Artificial Neuron\n",
    "- Gradient Descent\n",
    "- Backpropagation\n",
    "- Perceptron\n",
    "- Multilayered Perceptron\n",
    "- MLP Classification\n",
    "- **Model Design**\n",
    "- Optimization\n",
    "\n",
    "- Convolutional Neural Network\n",
    "- Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Design\n",
    "![Model Design](images/nn-model-design.png)\n",
    "- Input format\n",
    "- Output layer\n",
    "- Loss function(s)\n",
    "- Model Architecture\n",
    "- Optimization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Output layer\n",
    "\n",
    "### Regression\n",
    "\n",
    "* Linear units -> Gaussian output distributions\n",
    "    - Given a vector of feature activations $h$\n",
    "    - $\\hat{y}=W^Th+b$\n",
    "    - $p(y|x)=\\mathcal{N}(y;\\hat{y},I)$\n",
    "\n",
    "\n",
    "### Classification\n",
    "\n",
    "* Sigmoid units -> Bernoulli output distributions\n",
    "    - $p(y=1|x)$\n",
    "\n",
    "* Softmax units -> Multinoulli output distributions\n",
    "    * multiple neurons output the probability of each class\n",
    "    * Normalized with the softmax function\n",
    "    * $p(y=j|x)=\\frac{e^{\\mathbf{x}^\\top w_j}} {\\sum^{n}_{k=1}{e^{\\mathbf{x}^\\top w}}}$\n",
    "    * strictly positive\n",
    "    * sums to one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Linear Activation function\n",
    "\n",
    "$$g(z) = z$$\n",
    "$$h = g(W^\\top x + b)$$\n",
    "![Linear Activation](images/linear.png)\n",
    "- Usually used as a last layer activation for doing regression\n",
    "- If all neurons are linear, the MLP is linear, which limits the generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Logistic Sigmoid\n",
    "\n",
    "- $\\phi(z) = \\frac{1}{1 + e^{-z}}$\n",
    "- $h = \\phi(W^\\top x + b)$\n",
    "\n",
    "![Logit](images/sigmoid.png)\n",
    "\n",
    "- Positive, bounded, strictly increasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Hyperbolic Tangent\n",
    "\n",
    "- $\\phi(z) = \\tanh(z)$\n",
    "- $h = \\phi(W^\\top x + b)$\n",
    "![Hyperbolic Tangent](images/tanh.png)\n",
    "\n",
    "- Positive, negative, bounded, strictly increasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Rectified Linear Units\n",
    "- $\\phi(z) = max\\{0, z\\}$\n",
    "- $h = \\phi(W^\\top x + b)$\n",
    "![Relu](images/relu.png)\n",
    "\n",
    "* Bounded below by 0, no upper bound, monotonically increasing\n",
    "* Not differentiable at 0\n",
    "* Produces sparse activations\n",
    "* Addresses the vanishing gradient problem\n",
    "* Tip: Bias initialization to small positive values\n",
    "* Variations: Leaky ReLU, PReLU, Maxout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Depth and Width\n",
    "\n",
    "- Capacity\n",
    "- Compositional features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Number of layers\n",
    "\n",
    "#### Single hidden layer\n",
    "- Universal approximation theorem (Hornik, 1991)\n",
    "\n",
    "*\"a single hidden layer neural network with a linear output unit can approximate any continuous function arbitrarily well, given enough units\"*\n",
    "\n",
    "- Capacity scales poorly\n",
    "    - To learn a complex function the model needs exponentially many neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Shallow and deep network can learn the same functions\n",
    "- Models with sequence of layers:\n",
    "    - Each layer can partitioning the original input space piecewise linearly\n",
    "    - Each subsequent layer recognizes pieces of the original input\n",
    "    - Apply the same computation across different regions\n",
    "\n",
    "![folding the input space](images/folding-space.png)\n",
    "\n",
    "- The segments grows:\n",
    "    - exponentially with the number of layers \n",
    "    - polynomial with the number of neurons\n",
    "    \n",
    "- Should we use very deep networks for any problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "- Introduction and Motivation\n",
    "- Artificial Neuron\n",
    "- Gradient Descent\n",
    "- Backpropagation\n",
    "- Perceptron\n",
    "- Multilayered Perceptron\n",
    "- MLP Classification\n",
    "- Model Design\n",
    "- **Optimization**\n",
    "\n",
    "- Convolutional Neural Network\n",
    "- Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descenet\n",
    "\n",
    "* $\\theta \\gets \\theta - \\alpha \\nabla_\\theta L(\\mathbf{x}; \\theta)$\n",
    "\n",
    "- Overfitting\n",
    "    - Early stop, Learning rate adaptation\n",
    "    - Weight decay L1/L2 regularization (ridge regression)\n",
    "\n",
    "- Momentum\n",
    "    * $v \\gets \\gamma v - \\alpha \\nabla_\\theta L(\\mathbf{x}; \\theta)$\n",
    "    * $\\theta \\gets \\theta - v$\n",
    "\n",
    "- Nestorov momentum\n",
    "- AdatGrad\n",
    "- AdaDelta\n",
    "- Adam\n",
    "- RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "- Learn in Batches\n",
    "- Reduce learning rate when it plateaus\n",
    "    - Learning rate adaptation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/sgd-methods.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/sgd-methods2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/sgd-methods3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/sgd-methods4.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vanishing gradient\n",
    "\n",
    "![Logit](images/sigmoid.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Sigmoid activations 2 layers\n",
    "![Logit](images/vanish-grad-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Sigmoid activations 3 layers\n",
    "![Logit](images/vanish-grad-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Sigmoid activations 4 layers\n",
    "![Logit](images/vanish-grad-04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Sigmoid activations ReLU\n",
    "![Logit](images/vanish-grad-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization \n",
    "- L1/L2\n",
    "    - Weights\n",
    "    - Activations\n",
    "- Sparsity\n",
    "\n",
    "- https://keras.io/regularizers/\n",
    "- https://www.tensorflow.org/api_guides/python/contrib.layers#Regularizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initialization\n",
    "\n",
    "- Depends on the activation function\n",
    "    - ReLU, small postive weights\n",
    "    \n",
    "- (Grolot et al. 2010)\n",
    "- https://keras.io/initializers/\n",
    "- https://www.tensorflow.org/api_guides/python/contrib.layers#Initializers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout\n",
    "![Dropout](images/dropout.jpeg)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
